---
title: "Building a Classification Model for Credit Scoring using ML Techniques- Techniques Used: GLM, Trees, GAM,LDA and SVM "
output: 
  github_document :
    toc : true
    toc_depth : 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##References             

1. [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
2. [Elements of statistical learning](https://statweb.stanford.edu/~tibs/ElemStatLearn/)
3. [R Bloggers](https://www.r-bloggers.com/)

##Purpose of the Case Study          

German Credit dataset contains information about the default/non-default predictors across several industries.              

We fit supervised and classification models to the dataset consisting of 1000 observations and 21 variables.                          

The dataset contains 20 predictor variables i.e. Status of checking account, credit history, purpose, savings accounts/bonds, present employment since, personal status and sex, other debtors/guarantors, property, other installment plans, housing, job, telephone, foreign worker, duration in month, credit amount, installment rate in terms of percentage of disposable income, present resident since, age in years, number of existing credits, number of people liable to provide maintenance and 1 response variable: default or no default status.                   

The dataset is randomly sampled into 75% training and 25% testing data. A comparison based on different models based on different flexibilities, misclassification rates, area under the curve and mean residual deviance are made. We choose an asymmetric cost of 5:1 which is False Negative: False Positive.


## Packages Used and Data subsetting

The below packages are used for loading data, visualizations, model building and for creating creative plots of the results from each model.
```{r packages,warning=FALSE, message=FALSE}
library(ggplot2) 
library(car)
library(verification)
library(ROCR)
library(rpart)
library(rattle)
library(mgcv)
library(e1071)
library(reshape)
```

We set up the data using a random seed to sample the data into 75% training and 25% training data. We dont have sufficient data points to have a validation data as well.
```{r ,warning=FALSE, message=FALSE}
german_credit = 
  read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data")

colnames(german_credit) = c("chk_acct", "duration", "credit_his", 
                            "purpose", "amount", "saving_acct", 
                            "present_emp",   "installment_rate", 
                            "sex", "other_debtor", "present_resid", 
                            "property", "age", "other_install", 
                            "housing", "n_credits",  "job", 
                            "n_people", "telephone", "foreign", "response")

german_credit$response = german_credit$response - 1
set.seed(10857825)
subset <- sample(nrow(german_credit), nrow(german_credit) * 0.75)
credit.train = german_credit[subset, ]
credit.test = german_credit[-subset, ]
```

##Exploratory Data Analysis

Density Plots:                      

We use simple density plots to understand the difference of the distribution of data across the response of 0 and 1 across different numeric variable which shows an indication of predictive power.

```{r ,warning=FALSE, message=FALSE}
german_credit$response <- factor(german_credit$response)

ggplot(data = melt(german_credit), aes(x = value, color= response)) +
  geom_density(fill="white",alpha=0.55) +
  facet_wrap(~variable, scales = "free")
```

Distribution of Data:                    

A simple pie chart shows that most of the data is concentrated around good loans or non-defaulters with 0 as response (700 data points nearly).

```{r ,warning=FALSE, message=FALSE}
pie(table(german_credit$response),c("good","bad"))
```


Correlation Matrix:           

A simple correlation matrix in Figure 9 between the different numeric predictors is as shown below, we donâ€™t observe major correlation except between duration and amount.

```{r ,warning=FALSE, message=FALSE}
#Correlation Plots
c <- cor(german_credit[sapply(german_credit, is.numeric)], use="complete.obs", method="pearson")
c
```

##Logistic Regression

We fit generalized linear model in this case, it is logistic regression for classification using a linear relationship between the predictors and the response.             

We find that log-log link provides the best fit in terms of lowest AIC/BIC. We then fit using logit link on the GLM model and do model selection using stepwise with AIC and BIC and use the AIC model as it produces lesser deviance in the model. 


```{r ,warning=FALSE, message=FALSE}
#Logit function
creditmod <-glm(response~., family = binomial, credit.train)
AIC(creditmod)
BIC(creditmod)

#Probit Function
probitmod <- glm(response~., family = binomial(link="probit"), data=credit.train)
AIC(probitmod)
BIC(probitmod)

#Log Log Function
loglogmod<-glm(response~., family = binomial(link="cloglog"), data=credit.train)
AIC(loglogmod)
BIC(loglogmod)


#Model Selection-AIC
creditmod.step <- step(creditmod, trace=FALSE)
hist(predict(creditmod.step, type = "response"))
deviance(creditmod.step)

#Model Selection-BIC
creditmod.BIC <- step(creditmod, k = log(nrow(credit.train)),trace=FALSE)
hist(predict(creditmod.BIC, type = "response"))
deviance(creditmod.BIC)

```



In Sample : AUC: 0.83 Misclassification Rate: 0.37 Cost:0.50

```{r ,warning=FALSE, message=FALSE}
creditcost <- function(observed, predicted) {
    weight1 = 5
    weight0 = 1
    c1 = (observed == 1) & (predicted == 0)  #logical vector - true if actual 1 but predict 0
    c0 = (observed == 0) & (predicted == 1)  #logical vecotr - true if actual 0 but predict 1
    return(mean(weight1 * c1 + weight0 * c0))
}
#insample
prob.glm1.insample <- predict(creditmod.step, type = "response")
predicted.glm1.insample <- prob.glm1.insample > 0.1666
predicted.glm1.insample <- as.numeric(predicted.glm1.insample)
#Confusion Matrix
(table(credit.train$response, predicted.glm1.insample, dnn = c("Truth", "Predicted")))
#Error Rate
mean(ifelse(credit.train$response != predicted.glm1.insample, 1, 0))

roc.plot(credit.train$response == "1", prob.glm1.insample)
roc.plot(credit.train$response == "1", prob.glm1.insample)$roc.vol

creditcost(credit.train$response, predicted.glm1.insample)
```


Out of Sample : AUC: 0.76 Misclassification Rate: 0.38 Cost: 0.56

```{r ,warning=FALSE, message=FALSE}
#outsample
prob.glm1.outsample <- predict(creditmod.step, credit.test, type = "response")
predicted.glm1.outsample <- prob.glm1.outsample > 0.1666
predicted.glm1.outsample <- as.numeric(predicted.glm1.outsample)

#Confusion Matrix
table(credit.test$response, predicted.glm1.outsample, dnn = c("Truth", "Predicted"))

mean(ifelse(credit.test$response != predicted.glm1.outsample, 1, 0))



pred <- prediction(prob.glm1.outsample, credit.test$response)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE)
abline(a=0, b= 1)
auc.perf = performance(pred, measure = "auc")
auc.perf@y.values

creditcost(credit.test$response, predicted.glm1.outsample)
```


We find that the misclassification rate and costs are quite comparable proving a good underlying fit which indicates a linear relationship between the predictors and the response. We can compare this to other flexible and relatively inflexible models to understand the fit.                  

We perform cross-validation to find the optimal cut-off probability to choose and get 0.17, which is close to the original asymmetric cost chosen (5:1, therefore 1/6). The misclassification rate reduces to 0.37 in test data.



```{r ,warning=FALSE, message=FALSE}
#Cross- Validation
searchgrid = seq(0.01, 0.6, 0.02)
result = cbind(searchgrid, NA)
cost1 <- function(r, pi) {
  weight1 = 5
  weight0 = 1
  c1 = (r == 1) & (pi < pcut)  #logical vector - true if actual 1 but predict 0
  c0 = (r == 0) & (pi > pcut)  #logical vecotr - true if actual 0 but predict 1
  return(mean(weight1 * c1 + weight0 * c0))
}
credit.glm1 <- glm(response ~ chk_acct + duration + credit_his + purpose + amount + saving_acct + present_emp + installment_rate +other_debtor +  age + other_install + housing + foreign, family = binomial, german_credit)
for (i in 1:length(searchgrid)) {
  pcut <- result[i, 1]
  result[i, 2] <- cv.glm(data = german_credit, glmfit = credit.glm1, cost = cost1, 
                         K = 3)$delta[2]
}
plot(result, ylab = "CV Cost")


result[which.min((result[,2])),]
#ROC for cross validation
prob.glm1.insample <- predict(creditmod.step, type = "response")
predicted.glm1.insample <- prob.glm1.insample > 0.17
predicted.glm1.insample <- as.numeric(predicted.glm1.insample)
#Confusion Matrix
(table(credit.train$response, predicted.glm1.insample, dnn = c("Truth", "Predicted")))
#Error Rate
mean(ifelse(credit.train$response != predicted.glm1.insample, 1, 0))
library("verification")
roc.plot(credit.train$response == "1", prob.glm1.insample)
roc.plot(credit.train$response == "1", prob.glm1.insample)$roc.vol

```

##Trees

We fit classification trees using Gini index as the impurity index and plotted the tree with the following misclassification table.

```{r ,warning=FALSE, message=FALSE}

#Classification Trees
par(mfrow=c(1,1))
credit.rpart <- rpart(formula = response ~ . , data = credit.train, method = "class", 
                      parms = list(loss = matrix(c(0, 5, 1, 0), nrow = 2)))
fancyRpartPlot(credit.rpart)
```

In-Sample: AUC:  0.88 Misclassification Rate: 0.34 Cost: 0.35

```{r ,warning=FALSE, message=FALSE}
#Prediction using Classification Tree- In Sample
credit.train.pred.tree1 = predict(credit.rpart, type = "class")
table(credit.train$response, credit.train.pred.tree1, dnn = c("Truth", "Predicted"))
mean(ifelse(credit.train$response != credit.train.pred.tree1, 1, 0))
#ROC Curv-In Sample
credit.rpart2 <- rpart(formula = response ~ ., data = credit.train, method = "class", 
                       cp = 5e-04)
creditcost(credit.train$response, credit.train.pred.tree1)
```

Out of Sample: AUC:0.74  Misclassification Rate: 0.58 Cost: 0.50

```{r ,warning=FALSE, message=FALSE}
#Prediction using Classification Tree- Out Sample
credit.test.pred.tree1 = predict(credit.rpart, credit.test,type = "class")
table(credit.test$response, credit.test.pred.tree1, dnn = c("Truth", "Predicted"))
mean(ifelse(credit.train$response != credit.test.pred.tree1, 1, 0))
#ROC Curve-Out Sample
credit.rpart2 <- rpart(formula = response ~ ., data = credit.train, method = "class", 
                       cp = 5e-04)
##Creditcost
creditcost(credit.test$response, credit.test.pred.tree1)

```

We see that the testing data is showing more misclassifications and the training is overfitting the data and the tree might have to be pruned to get better results. 

## General Additive Model

We fit generalized additive model to capture some non-linear relationships for the numeric variables in the data using splines. The degrees of freedom of each of the spline depends on the combination of the covariates within each variable. 

```{r ,warning=FALSE, message=FALSE}
gam_formula <- as.formula(paste("response~s(duration)+s(amount)+s(age)"))
fitGAM <- gam(formula = gam_formula, family = binomial, data = credit.train)
summary(fitGAM)
par(mfrow=c(2,2))
plot(fitGAM, shade = TRUE,seWithMean = TRUE, scale = 0)
```

In-Sample: AUC:  0.65 Misclassification Rate: 0.30 Cost: 1.43

```{r ,warning=FALSE, message=FALSE}
##insample
fittedGAM.insample.prob <- predict(fitGAM)
fittedGAM.insample <- as.numeric(fittedGAM.insample.prob > 0.16)
table(credit.train$response, fittedGAM.insample, dnn = c("Observation", "Prediction"))
mean(ifelse(credit.train$response != fittedGAM.insample, 1, 0))
roc.plot(credit.train$response == "1", fittedGAM.insample.prob)
roc.plot(credit.train$response == "1", fittedGAM.insample.prob)$roc.vol 
creditcost(credit.train$response, fittedGAM.insample)
```

We find that the GAM fit is more linear than non-linear in fitting of the splines and fits a linear relationship for most of the variables except some variables, hence proving that a linear relationship maybe the best given the underlying data. We can also see that the cost is very much higher when compared to other linear models.

Out of Sample: AUC:0.70  Misclassification Rate: 0.28 Cost: 1.36


```{r ,warning=FALSE, message=FALSE}
##out-sample
fittedGAM.outsample.prob <- predict(fitGAM, credit.test)
fittedGAM.outsample <- as.numeric(fittedGAM.outsample.prob > 0.16)
table(credit.test$response, fittedGAM.outsample, dnn = c("Observation", "Prediction"))
mean(ifelse(credit.test$response != fittedGAM.outsample, 1, 0))
roc.plot(credit.test$response == "1", fittedGAM.outsample.prob)
roc.plot(credit.test$response == "1", fittedGAM.outsample.prob)$roc.vol 
creditcost(credit.test$response, fittedGAM.outsample)
```

##Linear Discriminant Analysis

We use linear discriminant analysis as a classification technique with certain assumptions, the underlying distribution is Gaussian or normal and the output has a linear relationship with the predictors. The performance of LDA depends on these given assumptions.


```{r ,warning=FALSE, message=FALSE}
fitLDA <- lda(response~ ., data = credit.train)
```

We fit LDA using the training data and find the following misclassification table.            
 
In-Sample: AUC:  0.84 Misclassification Rate: 0.35 Cost:0.49


```{r ,warning=FALSE, message=FALSE}
##insample 
lda.in <- predict(fitLDA)
pcut.lda <- 0.16
pred.lda.in <- (lda.in$posterior[, 2] >= pcut.lda) * 1
table(credit.train$response, pred.lda.in, dnn = c("Obs", "Pred"))
mean(ifelse(credit.train$response != pred.lda.in, 1, 0))
roc.plot(credit.train$response == "1", lda.in$posterior[, 2])
roc.plot(credit.train$response == "1", lda.in$posterior[, 2])$roc.vol 
creditcost(credit.train$response, pred.lda.in)
```


Out of Sample: AUC:0.78  Misclassification Rate: 0.37 Cost: 0.53


```{r ,warning=FALSE, message=FALSE}
##outsample 
lda.out <- predict(fitLDA, newdata = credit.test)
cut.lda <- 0.16
pred.lda.out <- as.numeric((lda.out$posterior[, 2] >= cut.lda))
table(credit.test$response, pred.lda.out, dnn = c("Obs", "Pred"))
mean(ifelse(credit.test$response != pred.lda.out, 1, 0)) 
roc.plot(credit.test$response == "1", lda.out$posterior[, 2])
roc.plot(credit.test$response == "1", lda.out$posterior[, 2])$roc.vol 
creditcost(credit.test$response, pred.lda.out)
```

We find that the performance between test and training data to be comparable and hence a good indicator of the underlying relationship between the predictors and response might be linear and may follow the assumptions made by LDA.


## Support Vector Machines

Support vector machine (SVM) is touted as one of the best off-the-shelf classifying algorithm. It handles non-linearity, well-regularized, has very few parameters(support vectors) and is very fast.     

SVM algorithm works on finding a hyperplane which will maximize the distance between the hyperplane and the support vectors.                 

The formulation of the hyperplane can be both linear and non-linear. We use SVM on our data to fit a classifying hyperplane to divide between the binary response.


```{r ,warning=FALSE, message=FALSE}
credit.svm = svm(response ~ ., data = credit.train, cost = 1, gamma = 1/length(credit.train), 
                 probability = TRUE)
```

In-Sample: AUC:  0.92 Misclassification Rate: 0.17 Cost:0.31


```{r ,warning=FALSE, message=FALSE}
prob.svm = predict(credit.svm, credit.train, probability = TRUE)
pred.svm = as.numeric((prob.svm >= 0.16))
table(credit.train$response, pred.svm, dnn = c("Obs", "Pred"))
mean(ifelse(credit.train$response != pred.svm, 1, 0))
creditcost(credit.train$response, pred.svm)

roc.plot(credit.train$response == "1", prob.svm)
roc.plot(credit.train$response == "1", prob.svm)$roc.vol 
```


Out of Sample: AUC:0.75  Misclassification Rate: 0.33 Cost: 0.60

```{r ,warning=FALSE, message=FALSE}
prob.svm = predict(credit.svm, credit.test, probability = TRUE)
pred.svm = as.numeric((prob.svm >= 0.16))
table(credit.test$response, pred.svm, dnn = c("Obs", "Pred"))
mean(ifelse(credit.test$response != pred.svm, 1, 0))
creditcost(credit.test$response, pred.svm)
roc.plot(credit.test$response == "1", prob.svm)
roc.plot(credit.test$response == "1", prob.svm)$roc.vol 
```

We find that SVM over-fits in the training dataset leading to a marked increase in misclassification rate and costs associated. 
